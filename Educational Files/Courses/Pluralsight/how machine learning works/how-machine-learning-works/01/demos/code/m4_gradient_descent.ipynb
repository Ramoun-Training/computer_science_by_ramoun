{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's load pizzas and reservations from `pizza.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the `predict()` and `loss()` functions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    return X * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y, w, b):\n",
    "    predictions = predict(X, w, b)\n",
    "    return np.average((predictions - Y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set `b` at 0, so that we have a 2-dimensional chart, and let's visualize how the loss changes as `w` changes. (By the way, don't worry about understanding the plotting code, unless you want to. This code isn't a requisite to follow this training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw matplotlib plots inside this notebook:\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute losses for w ranging from -1 to 4:\n",
    "weights = np.linspace(-1.0, 4.0, 200)\n",
    "losses = [loss(X, Y, w, 0) for w in weights]\n",
    "\n",
    "# Plot weights and losses:\n",
    "plt.axis([-1, 4, 0, 1000])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(weights, losses, color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we also consider `b`, then the loss becomes a surface, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Prepare matrices for 3D plot (W, B and L for weights, biases and losses):\n",
    "MESH_SIZE = 20\n",
    "weights = np.linspace(-1, 4, MESH_SIZE)\n",
    "biases = np.linspace(-20, 20, MESH_SIZE)\n",
    "W, B = np.meshgrid(weights, biases)\n",
    "losses = np.array([loss(X, Y, w, b) for w, b in zip(np.ravel(W), np.ravel(B))])\n",
    "L = losses.reshape((MESH_SIZE, MESH_SIZE))\n",
    "\n",
    "# Plot surface:\n",
    "ax = plt.figure().gca(projection=\"3d\")\n",
    "ax.set_xlabel(\"Weight\", labelpad=15)\n",
    "ax.set_ylabel(\"Bias\", labelpad=15)\n",
    "ax.set_zlabel(\"Loss\", labelpad=15)\n",
    "ax.plot_surface(W, B, L, cmap=cm.gnuplot, antialiased=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement gradient descent–and algorithm that descends the gradient of this curve. First, let's write a function to calculate the gradient. In mathematical formulae, the loss looks like this:\n",
    "\n",
    "\\begin{align*}\n",
    "L = \\frac{1}{m}\\sum\\limits_{i} ((wx_i + b) - y_i) ^ 2\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of this loss is the composition of the derivatives of `L` with respect to `w` and `b`. If you know calculus, then you can calculate those derivatives on your own. If you don't, then here they are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{m}\\sum\\limits_{i} 2x((wx_i + b) - y_i)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{m}\\sum\\limits_{i} 2((wx_i + b) - y_i)\n",
    "\\end{align*}\n",
    "\n",
    "And here is a function that returns both derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Y, w, b):\n",
    "    w_gradient = np.average(2 * X * (predict(X, w, b) - Y))\n",
    "    b_gradient = np.average(2 * (predict(X, w, b) - Y))\n",
    "    return (w_gradient, b_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(X, Y, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `gradient()`, we can update `train()` to do gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteration %4d => Loss: %.6f\" % (i, loss(X, Y, w, b)))\n",
    "        w_gradient, b_gradient = gradient(X, Y, w, b)\n",
    "        w -= w_gradient * lr\n",
    "        b -= b_gradient * lr\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `train()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = train(X, Y, 10000, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters `w` and `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, here are the pizzas we can expect to sell for 42 reservations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservations = 42\n",
    "predict(reservations, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we round this up to the nearest integer, this calculation based on gradient descent came up with 59 pizzas, instead of the 60 that we calculated with the previous algorithm–and it was also faster!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
